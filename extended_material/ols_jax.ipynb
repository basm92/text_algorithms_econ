{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab: \n",
    "# 1. First CHANGE RUNTIME TYPE to GPU \n",
    "# 2. Then run install commands commented out below\n",
    "# 3. Then RESTART RUNTIME\n",
    "# 4. Then run git clone command commented out below\n",
    "# 5. Then run all the other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install --upgrade jax==0.2.3 jaxlib==0.1.56+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "# !pip install --upgrade numpyro==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart RUNTIME after installing packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://USER_NAME:ACCESS_TOKEN@PATH_TO_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd mres_methods_course/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression Models with JAX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import random, vmap, jit\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey as Key\n",
    "\n",
    "import numpyro\n",
    "numpyro.set_platform(\"cpu\")\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(jax.lib.xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce basic ideas in JAX, a relatively recent Python library developed by Google for writing Python functions that can be automatically differentiated, evaluated on batch data, and compiled to run on modern hardware like GPUs and TPUs.  This allows programmers who already are familiar with the core Python data science packages to prototype and efficiently estimate large-scale machine learning models.\n",
    "\n",
    "For an introductory video see https://www.youtube.com/watch?v=WdTeDXsOSj4&ab_channel=TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return(7*x**3 + x**2)\n",
    "\n",
    "def h(x):\n",
    "    return(jnp.sin(x) * jnp.cos(x) + f(jnp.sqrt(x)) / jnp.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives examples of two separate functions written in Python.  The first is very simple $f(x) = 7x^3 + x^2$ while the second is more complicated $h(x) = \\sin(x)\\cos(x) + f(\\sqrt{x} / \\exp(x))$.  Notice that $f$ is written in standard Python, whereas $h$ uses native JAX functions for evaluating functions like sine, square root, and so on.  The API is quite similar to Numpy where such function would be invoked using ```np.sin(x)```, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0\n",
      "86.0\n",
      "42.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(jax.grad(f)(2.0)) # first derivative of f evaluated at 2\n",
    "print(jax.grad(jax.grad(f))(2.0)) # second derivative of f\n",
    "print(jax.grad(jax.grad(jax.grad(f)))(2.0)) # third derivative of f\n",
    "print(jax.grad(jax.grad(jax.grad(jax.grad(f))))(2.0)) # fourth derivative of f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written these functions, we would like to differentiate them.  This can be done within JAX using the ```grad``` operation.  Notice that ```grad``` is composoable and can be called on itself to compute higher-order derivates.  The above code illustrates this on the simple function, while the code below illustrates this on the more complex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.4588542\n",
      "0.6762605\n",
      "4.4664674\n",
      "-8.073041\n"
     ]
    }
   ],
   "source": [
    "print(jax.grad(h)(2.0))\n",
    "print(jax.grad(jax.grad(h))(2.0))\n",
    "print(jax.grad(jax.grad(jax.grad(h)))(2.0))\n",
    "print(jax.grad(jax.grad(jax.grad(jax.grad(h))))(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustrations above show derivatives computed for functions that map scalars into scalars.  ```grad``` can only operate on functions that return scalar values, but the inputs can be multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22. 66.]\n"
     ]
    }
   ],
   "source": [
    "def f(x, alpha):\n",
    "    \n",
    "    x_0 = x[0]\n",
    "    x_1 = x[1]\n",
    "    \n",
    "    y = (x_0 + alpha * x_1)**2\n",
    "    \n",
    "    return(y)\n",
    "\n",
    "grad_example1 = jax.grad(f)\n",
    "alpha = 3\n",
    "print(grad_example1(jnp.array([2.0, 3.0]), alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we need not explictly unpack vector inputs.  ```grad``` is aware of the dimensionality of the input we pass, and computes the gradients accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "6.0\n",
      "[10. 10.]\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    \n",
    "    y = jnp.outer(x, x)\n",
    "    return(jnp.sum(y))\n",
    "\n",
    "\n",
    "grad_example2 = jax.grad(h)\n",
    "print(grad_example2(jnp.array(2.0)))\n",
    "print(grad_example2(jnp.array(3.0)))\n",
    "print(grad_example2(jnp.array([2.0, 3.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take this basic idea of the automatic differentiation of user-defined functions to show how to recover the parameters of simple regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Synthetic Data Generation**\n",
    "\n",
    "To begin our illustration, we simulate data from a multivariate normal distribution with spherical errors whose mean depends on the linear combination of various features.  This is the data generating process that describes the familar OLS model $y_i = \\beta_0 + \\mathbf{x}_i^T\\boldsymbol\\beta + \\varepsilon_i$ where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.  In vector form the model becomes:\n",
    "$\\begin{equation*}\n",
    "   \\mathbf{y}|\\mathbf{x} \\sim N(\\beta_0 + \\mathbf{x}^T\\boldsymbol\\beta, I\\sigma^2)\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(key, n_obs, n_features, betas, noise_std=1, intercept=True):\n",
    "    \"\"\" Function to generate data for a regression task. If intercept = True, then\n",
    "        the first value of beta would be the intercept of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    key1, key2 = jax.random.split(key, 2)\n",
    "    X = dist.Normal(0, 4).expand([n_obs, n_features]).sample(key1)\n",
    "\n",
    "    if intercept:\n",
    "        X = jnp.hstack([X, jnp.ones(shape=(n_obs,1))])\n",
    "    \n",
    "    noise = dist.Normal(0,noise_std).expand([n_obs,]).sample(key2)\n",
    "    y = betas@X.T + noise\n",
    "    \n",
    "    return y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "n_obs = 1000\n",
    "n_features = 5\n",
    "noise_std = 8\n",
    "betas = jnp.array([4,3,2,6,0,0]) # first element is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 6), (1000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "y, X = gen_data(Key(201), n_obs=n_obs, n_features=n_features, \n",
    "                betas=betas, noise_std=noise_std, intercept=True)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Ordinary Least Squares Regression with JAX**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the standard OLS model, we choose a vector of regression coefficients $\\boldsymbol\\beta$ to minimize the following loss function:\n",
    "\\begin{align*}\n",
    "L = \\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta^T\\textbf{x}_i)^2}{N}\n",
    "\\end{align*}\n",
    "The solution to this problem has a well-known analytic form $\\hat{\\boldsymbol\\beta}^{\\textrm{OLS}} = (X^T X)^{-1}X^T y$.  Here we use gradient descent to minimize the loss, where we illustrate basic operations in JAX for efficient automatic differentiation.\n",
    "\n",
    "We begin by defining a function for computing the squared error between an observed data point $y_i$ and its predicted mean value $ \\mathbf{x}_i^T\\boldsymbol\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_obs(params, y, x):\n",
    "    \"\"\"calculate the squared error for a single observation\"\"\"\n",
    "    pred = x@params\n",
    "    return (y - pred)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate this function on a particular data point from our simulated data drawn above.  \n",
    "\n",
    "We first randomly initialize parameter values for the regression coefficients by drawing them from standard normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "J = X.shape[1] # number of covariates in model\n",
    "key = Key(92) # key for random number generator; different integer value produce different random draws\n",
    "params = jax.random.normal(key, shape=(J, ))\n",
    "print(params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now evaluate the squared error at a data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(270.7514, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "squared_error_obs(params, y[4], X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Vectorization*\n",
    "\n",
    "To compute the squared error over all data points, and thus construct the OLS loss function, we need to evaluate the above function across all observations.  JAX's *vmap* transforms a function written to process only one observation into a function that can handle multiple observations.  The single line of code below creates a new function that applies our simple squared error function to all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared_error_obs(params, y, x)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vectorized version of squared error. Note that vmap returns a function\n",
    "complete_squared_error = jax.vmap(squared_error_obs, in_axes=(None, 0, 0))\n",
    "complete_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call ```vmap``` we need to specify over which data we are iterating.  **None** in the above specifies that params stays fixed across function evaluations.  The 0 values imply we will evaluate the function across the first axis of $y$ and $X$.  This is redundant in the case of $y$ since it only has one access.  For $X$ this means we evaluate across rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the vmapped function with all the observations\n",
    "y_hats = complete_squared_error(params, y, X)\n",
    "y_hats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the standard OLS loss function by computing the mean value of the vector of squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, y, X):\n",
    "    \"\"\" Function to compute the loss for multiple observations\"\"\"\n",
    "    \n",
    "    # calculate the mean-squared error for the given observations\n",
    "    return jnp.mean(complete_squared_error(params, y, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160.0277\n"
     ]
    }
   ],
   "source": [
    "# get the value of the loss for all the observations\n",
    "print(loss(params, y, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradients*\n",
    "\n",
    "Above we illustrated how ```grad``` can be applied to Python functions to compute derivates automatically.  Here we can apply this idea to the loss function above to obtain its gradient.  Instead of applying ```grad``` we here instead apply ```value_and_grad``` which returns the value of the function in addition to its gradient.  The ```argnums``` variable passed in the invocation of ```value_and_grad``` specifies the positions of the arguments in the function with respect to which we wish to automatically differentiate.  In our case, the relevant parameters are the regression coefficients which lie in the 0th position in the ```loss``` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.loss(params, y, X)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create version of the loss function that can calculate gradients\n",
    "loss_grad = jax.value_and_grad(loss, argnums=0)\n",
    "loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160.0277\n",
      "[-106.195816   -56.674194   -28.216362  -235.28566    -14.93482\n",
      "    1.9265889]\n"
     ]
    }
   ],
   "source": [
    "# explore the new function\n",
    "loss_value, gradient_loss = loss_grad(params, y, X)\n",
    "\n",
    "# loss evaluated at (params, y, X)\n",
    "print(loss_value)\n",
    "\n",
    "# gradient evaluated at (params, y, X)\n",
    "print(gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradient Descent*\n",
    "Now that we are able to compute the gradient of the loss function we can optimize it with respect to the regression parameters.  To do so we use a simple gradient descent algorithm.  First we compute the derivate of the loss function evaluated at the current value of the regression coefficients.  Then we update the regression coefficients by moving them a small amount in the opposite direction of the gradient.  The following function defines this update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, y, X, learning_rate):\n",
    "    \"\"\" function to update the parameters of the model based on the gradient\n",
    "        of the loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the value and gradient of the loss function\n",
    "    loss, grads = loss_grad(params, y, X)\n",
    "    \n",
    "    # update the parameters of the model by descending on the gradient\n",
    "    new_params = params - learning_rate*grads\n",
    "    \n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we begin from the randomly generated initial parameters above, and update them 1,000 times using gradient descent.  The code below implements the iterations and tracks the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 1160.0277099609375\n",
      "Loss in epoch 100: 67.12152862548828\n",
      "Loss in epoch 200: 65.51961517333984\n",
      "Loss in epoch 300: 65.49553680419922\n",
      "Loss in epoch 400: 65.48104858398438\n",
      "Loss in epoch 500: 65.4713363647461\n",
      "Loss in epoch 600: 65.46481323242188\n",
      "Loss in epoch 700: 65.46045684814453\n",
      "Loss in epoch 800: 65.45753479003906\n",
      "Loss in epoch 900: 65.45555877685547\n",
      "Loss in epoch 1000: 65.45423889160156\n",
      "Loss in epoch 1100: 65.453369140625\n",
      "Loss in epoch 1200: 65.45276641845703\n",
      "Loss in epoch 1300: 65.4523696899414\n",
      "Loss in epoch 1400: 65.45211029052734\n",
      "Loss in epoch 1500: 65.45193481445312\n",
      "Loss in epoch 1600: 65.45182037353516\n",
      "Loss in epoch 1700: 65.45174407958984\n",
      "Loss in epoch 1800: 65.45169067382812\n",
      "Loss in epoch 1900: 65.4516372680664\n",
      "Total training time: 15.8 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets apply the update function iteratively (several epochs)\n",
    "start = time.time()\n",
    "learning_rate = 0.001 # the small amount by which we adjust the regression coefficients in each update\n",
    "num_epochs = 2000\n",
    "\n",
    "params = jax.random.normal(key, shape=(J, ))\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss, params = update(params, y, X, learning_rate)\n",
    "    losses.append(loss)\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Loss in epoch {epoch}: {loss}\")\n",
    "        \n",
    "training_duration = np.round(time.time() - start, 2)\n",
    "print(f\"Total training time: {training_duration} seconds\")\n",
    "# plot the loss\n",
    "plt.plot(list(range(num_epochs)), losses[0:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True betas: [4 3 2 6 0 0] \n",
      "Gradient Descent betas: [ 3.9455807   2.9163518   1.9742827   5.965656    0.02850169 -0.4015602 ]\n"
     ]
    }
   ],
   "source": [
    "# compare the estimates to the real parameters\n",
    "print(f'True betas: {betas} \\nGradient Descent betas: {params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to exact coefficients from OLS regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.9455304   2.9163246   1.9742746   5.965641    0.02854469 -0.4085737 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,y))) # numpy for (X^X)^{-1}X^Ty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Just-In-Time (JIT) compilation*\n",
    "\n",
    "JAX provides and additional functionality that allows significant speed-up in the execution of the code. ```jit``` takes advantage of [XLA](https://www.tensorflow.org/xla/) in order to efficiently compile code that is going to be used multiple times. Our ```loss_grad``` function is, thus, a perfect candidate on which to use this functionality; it is a function that will be compiled in each one of the training epochs. ```jit``` can be called both by using ```jax.jit()``` or by using a decorator ```@jit```  and can be composed with all the other functions we have seen (e.g. ```jax.vmap```).\n",
    "\n",
    "Using JIT, though, brings additional considerations to the structuring of functions. For instance, in order to efficiently compile the code, JIT needs to know the shape of the result of the function only by looking at the shape of the parameters. In order words, inside a \"jitted\" function there cannot be operations that modify the shape of the result based on the value of the parameters. Another restriction when using JIT, is that functions cannot be passed as parameters to the function. We will have to modify our original loss function to comply with this.\n",
    "\n",
    "Some of these considerations are explained in the [JAX documentaion](https://jax.readthedocs.io/en/latest/errors.html#jax._src.errors.ConcretizationTypeError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of the loss function\n",
    "@jit\n",
    "def loss(params, y, X):\n",
    "    \"\"\" Function to compute the loss for multiple observations\"\"\"\n",
    "    \n",
    "    # calculate the mean-squared error for the given observations\n",
    "    return jnp.mean(complete_squared_error(params, y, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.loss(params, y, X)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss with gradient and jit\n",
    "loss_grad_jit = jax.jit(jax.value_and_grad(loss, argnums=0))\n",
    "loss_grad_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, y, X, learning_rate):\n",
    "    \"\"\" function to update the parameters of the model based on the gradient\n",
    "        of the loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the value and gradient of the loss function\n",
    "    loss, grads = loss_grad_jit(params, y, X)\n",
    "    \n",
    "    # update the parameters of the model by descending on the gradient\n",
    "    new_params = params - learning_rate*grads\n",
    "    \n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 1160.02783203125\n",
      "Loss in epoch 100: 67.12153625488281\n",
      "Loss in epoch 200: 65.51962280273438\n",
      "Loss in epoch 300: 65.49553680419922\n",
      "Loss in epoch 400: 65.4810562133789\n",
      "Loss in epoch 500: 65.47134399414062\n",
      "Loss in epoch 600: 65.46481323242188\n",
      "Loss in epoch 700: 65.46045684814453\n",
      "Loss in epoch 800: 65.45753479003906\n",
      "Loss in epoch 900: 65.45555877685547\n",
      "Loss in epoch 1000: 65.4542465209961\n",
      "Loss in epoch 1100: 65.453369140625\n",
      "Loss in epoch 1200: 65.45277404785156\n",
      "Loss in epoch 1300: 65.45237731933594\n",
      "Loss in epoch 1400: 65.45211791992188\n",
      "Loss in epoch 1500: 65.45193481445312\n",
      "Loss in epoch 1600: 65.45182037353516\n",
      "Loss in epoch 1700: 65.45175170898438\n",
      "Loss in epoch 1800: 65.45169067382812\n",
      "Loss in epoch 1900: 65.45164489746094\n",
      "Total training time: 1.4 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets apply the update function iteratively (several epochs)\n",
    "start = time.time()\n",
    "learning_rate = 0.001 # the small amount by which we adjust the regression coefficients in each update\n",
    "num_epochs = 2000\n",
    "\n",
    "params = jax.random.normal(key, shape=(J, ))\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss, params = update(params, y, X, learning_rate)\n",
    "    losses.append(loss)\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Loss in epoch {epoch}: {loss}\")\n",
    "        \n",
    "training_duration_jit = np.round(time.time() - start, 2)\n",
    "print(f\"Total training time: {training_duration_jit} seconds\")\n",
    "# plot the loss\n",
    "plt.plot(list(range(num_epochs)), losses[0:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time without JIT: 15.8 seconds\n",
      "Training time with JIT: 1.4 seconds\n"
     ]
    }
   ],
   "source": [
    "# compare the training times\n",
    "print(f\"Training time without JIT: {training_duration} seconds\")\n",
    "print(f\"Training time with JIT: {training_duration_jit} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Ridge Regression with JAX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Ridge regression we minimize the following loss:\n",
    "\\begin{align}\n",
    "L = \\sum_{i=1}^N (y_i - \\beta_0 - \\beta^T\\textbf{x}_i)^2 + \\gamma\\sum_{j=0}^K\\beta_j^2 \n",
    "\\end{align}\n",
    "We now modify the above loss to take into account the regularization term.  Otherwise, all other steps are identical as for OLS and we can easily recover Ridge estimates.\n",
    "\n",
    "Note that in the loss function we continue to define the mean squared error rather than the sum of squared errors.  This is equivalent to running a Ridge regression with a penalty term of $N \\gamma$, and is done to normalize the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_ridge(params, y, X, gamma):\n",
    "    \"\"\" Function to compute the loss for multiple observations\"\"\"\n",
    "    \n",
    "    # calculate the mean-squared error for the given observations\n",
    "    mse = jnp.mean(complete_squared_error(params, y, X))\n",
    "    \n",
    "    # scale gamma by the number of observations\n",
    "    return mse + gamma*jnp.sum(params**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ridge_grad_jit = jax.jit(jax.value_and_grad(loss_ridge, argnums=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, y, X, learning_rate, gamma):\n",
    "    \"\"\" function to update the parameters of the model based on the gradient\n",
    "        of the loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the value and gradient of the loss function\n",
    "    loss, grads = loss_ridge_grad_jit(params, y, X, gamma)\n",
    "    \n",
    "    # update the parameters of the model by descending on the gradient\n",
    "    new_params = params - learning_rate*grads\n",
    "    \n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 1168.2406005859375\n",
      "Loss in epoch 100: 179.16357421875\n",
      "Loss in epoch 200: 178.5372314453125\n",
      "Loss in epoch 300: 178.5330352783203\n",
      "Loss in epoch 400: 178.53192138671875\n",
      "Loss in epoch 500: 178.53158569335938\n",
      "Loss in epoch 600: 178.53147888183594\n",
      "Loss in epoch 700: 178.5314483642578\n",
      "Loss in epoch 800: 178.5314483642578\n",
      "Loss in epoch 900: 178.5314483642578\n",
      "Loss in epoch 1000: 178.53143310546875\n",
      "Loss in epoch 1100: 178.53143310546875\n",
      "Loss in epoch 1200: 178.53143310546875\n",
      "Loss in epoch 1300: 178.5314178466797\n",
      "Loss in epoch 1400: 178.53143310546875\n",
      "Loss in epoch 1500: 178.53143310546875\n",
      "Loss in epoch 1600: 178.53143310546875\n",
      "Loss in epoch 1700: 178.53143310546875\n",
      "Loss in epoch 1800: 178.53143310546875\n",
      "Loss in epoch 1900: 178.53143310546875\n",
      "Total training time: 15.8 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD5CAYAAADLL+UrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXSElEQVR4nO3df5Bd5X3f8ff33ruSkPghCS0US0okF9kJk04D3VLiOG4SUgzUsWgaOjCeQYOZYTJDGru0U3A9UzLtTBsS105oOmSIoREdCnYdZ1A6dgwFXKczhVjC/LTAkjGGjWS0sUAGZP1Y6ds/7rOruz+l3bt7r3rO+zWzc899znPvee65dz/32ec852xkJpKkemj0uwGSpN4x9CWpRgx9SaoRQ1+SasTQl6QaMfQlqUZaJ6sQEfcBHwH2ZebPlLLfA34VOAJ8F7gxM98q6z4F3AQcA34rM79Wyq8E/gBoAp/PzN852bbXrFmTGzZsmMfLkqT62rFjx99k5uB06+Jk8/Qj4kPAO8D9HaF/BfB4Zo5GxJ0AmXlbRFwEPAhcCrwH+F/A+8pTfQf4R8Aw8E3g+sz89mzbHhoayu3bt5/aq5QkARAROzJzaLp1Jx3eycxvAPsnlT2SmaPl7pPAurK8GXgoMw9n5veA3bS/AC4FdmfmK5l5BHio1JUk9dBCjOl/HPhqWV4LvN6xbriUzVQuSeqhrkI/Ij4NjAIPjBVNUy1nKZ/uOW+OiO0RsX1kZKSb5kmSJpl36EfEFtoHeD+WJw4MDAPrO6qtA/bMUj5FZt6TmUOZOTQ4OO1xCEnSPM0r9MtMnNuAj2bmwY5V24DrImJpRGwENgF/RfvA7aaI2BgRS4DrSl1JUg+dypTNB4FfBNZExDBwB/ApYCnwaEQAPJmZv5GZL0bEF4Fv0x72uSUzj5Xn+U3ga7SnbN6XmS8uwuuRJM3ipFM2+8kpm5I0d11N2fz/0buHR/nsIy/zrdfe7HdTJOm0UsnQP3T0GHc9vpvnhg/0uymSdFqpZOg3G+0ZoseOn75DV5LUD5UO/eOn8fEKSeqHSof+qD19SZqgkqHfCId3JGk6lQz98eEdQ1+SJqhm6I/19B3Tl6QJKhn6DXv6kjStSoY+QKsRHsiVpEkqG/qNRji8I0mTVDb0mxEO70jSJNUN/UZw7Hi/WyFJp5fKhn4jPCNXkiarbOi3mg1PzpKkSSob+o1w9o4kTVbZ0G82nKcvSZNVN/TDKZuSNFllQ7/RcMqmJE1W2dBvenKWJE1R6dD3QK4kTVTd0PeMXEmaorqh3wjn6UvSJJUN/UaEZ+RK0iSVDX17+pI0VWVDv31p5X63QpJOL5UN/VYjOHbcy2xKUqfKhn4zHN6RpMkqG/qNBtjRl6SJKhv6npErSVNVNvQbDu9I0hQnDf2IuC8i9kXECx1lqyPi0YjYVW5XlfKIiLsiYndEPBcRl3Q8ZkupvysitizOyzmh1XCeviRNdio9/T8BrpxUdjvwWGZuAh4r9wGuAjaVn5uBu6H9JQHcAfwD4FLgjrEvisXSbASjztmUpAlOGvqZ+Q1g/6TizcDWsrwVuKaj/P5sexJYGREXAB8GHs3M/Zn5JvAoU79IFpRn5ErSVPMd0z8/M/cClNvzSvla4PWOesOlbKbyReMZuZI01UIfyI1pynKW8qlPEHFzRGyPiO0jIyPzbkjD2TuSNMV8Q/+NMmxDud1XyoeB9R311gF7ZimfIjPvycyhzBwaHBycZ/O8tLIkTWe+ob8NGJuBswV4uKP8hjKL5zLgQBn++RpwRUSsKgdwryhli6blP1GRpClaJ6sQEQ8CvwisiYhh2rNwfgf4YkTcBLwGXFuqfwW4GtgNHARuBMjM/RHx74Fvlnr/LjMnHxxeUP6PXEma6qShn5nXz7Dq8mnqJnDLDM9zH3DfnFrXhWY4pi9Jk1X3jNxGcMxr70jSBJUN/WYD5+lL0iSVDf1Wo+E8fUmapLKh7wXXJGmqyoZ+s4GhL0mTVDb0PSNXkqaqbOh7Rq4kTVXd0LenL0lTVDr0M7G3L0kdqhv60b6wp719STqhsqHfaJTQt6cvSeMqG/rNEvqelStJJ1Q39MOeviRNVtnQHxveOe5F1yRpXGVDv1VCf9TUl6RxlQ398QO5julL0rjKhv7YmL4dfUk6obqhX16ZPX1JOqGyod8Y7+kb+pI0prKh32o6ZVOSJqts6I/19EcNfUkaV9nQ94xcSZqquqE/1tM/ZuhL0pjKhn6rTN+xpy9JJ1Q39MvwztFjTtSXpDHVDf2mB3IlabLqhn6j/dIc05ekE6ob+k0vuCZJk1U39BvO3pGkySoc+mV4xzF9SRpX3dAfG95x9o4kjesq9CPiX0TEixHxQkQ8GBHLImJjRDwVEbsi4gsRsaTUXVru7y7rNyzEC5jJgLN3JGmKeYd+RKwFfgsYysyfAZrAdcCdwOcycxPwJnBTechNwJuZeSHwuVJv0TTHh3fs6UvSmG6Hd1rAGRHRApYDe4FfBr5U1m8FrinLm8t9yvrLI8q1EhbBiZOz7OlL0ph5h35m/jXwGeA12mF/ANgBvJWZo6XaMLC2LK8FXi+PHS31z53v9k/GSytL0lTdDO+sot173wi8B1gBXDVN1bHUna5XPyWRI+LmiNgeEdtHRkbm27yOk7Mc3pGkMd0M7/wK8L3MHMnMo8CXgQ8AK8twD8A6YE9ZHgbWA5T15wD7Jz9pZt6TmUOZOTQ4ODjvxnkgV5Km6ib0XwMui4jlZWz+cuDbwBPAr5c6W4CHy/K2cp+y/vHMxbsEZtOTsyRpim7G9J+ifUD2aeD58lz3ALcBt0bEbtpj9veWh9wLnFvKbwVu76LdJzVQLq181Nk7kjSudfIqM8vMO4A7JhW/Alw6Td1DwLXdbG8uxmbvHLOnL0njKntG7tjwzlHH9CVpXGVDPyJoNcLZO5LUobKhD+3evvP0JemESof+QLPhGbmS1KHSod9qBsecvSNJ46od+o3wQK4kdah46Dc8kCtJHSod+s1GeBkGSepQ6dAfaIaXYZCkDpUO/Vaz4ZRNSepQ7dBvBEcd05ekcdUO/aZj+pLUqdqh32gY+pLUoeKh77V3JKlTtUPf2TuSNEG1Q7/RYNTLMEjSuGqHvgdyJWmCaod+o+HwjiR1qHjoh8M7ktSh2qHvgVxJmqDaoe8F1yRpgmqHftNLK0tSp0qH/oCzdyRpgkqHvtfTl6SJKh36rUbDq2xKUodKh/6SlqEvSZ2qHfrNBkdGDX1JGlPp0B9oNjie+N+zJKmodOgvabVfnr19SWoz9CWpRqod+s0A4IgHcyUJ6DL0I2JlRHwpIl6KiJ0R8XMRsToiHo2IXeV2VakbEXFXROyOiOci4pKFeQkzG+/pG/qSBHTf0/8D4C8y86eAvwvsBG4HHsvMTcBj5T7AVcCm8nMzcHeX2z4ph3ckaaJ5h35EnA18CLgXIDOPZOZbwGZga6m2FbimLG8G7s+2J4GVEXHBvFt+Cgaa7ZfnXH1Jauump/9eYAT4rxHxrYj4fESsAM7PzL0A5fa8Un8t8HrH44dL2aJZ0rSnL0mdugn9FnAJcHdmXgy8y4mhnOnENGVTJtBHxM0RsT0ito+MjHTRvBPDO4cNfUkCugv9YWA4M58q979E+0vgjbFhm3K7r6P++o7HrwP2TH7SzLwnM4cyc2hwcLCL5p0IfYd3JKlt3qGfmT8AXo+I95eiy4FvA9uALaVsC/BwWd4G3FBm8VwGHBgbBlosDu9I0kStLh//z4EHImIJ8ApwI+0vki9GxE3Aa8C1pe5XgKuB3cDBUndROXtHkibqKvQz8xlgaJpVl09TN4FbutneXDlPX5ImqvQZuU7ZlKSJKh36Y2P6zt6RpLZKh/5Sx/QlaYJKh77DO5I0UaVD39k7kjSRoS9JNVLp0G812ld+cHhHktoqHfoRwZJWg8OGviQBFQ99gKXNhsM7klRUPvQHWg2HdySpqHzoL7GnL0njqh/6LUNfksbUI/Qd3pEkoAahP+DwjiSNq3zoLxtoeME1SSqqH/qtJoeOHut3MyTptFD50D9jSZMfG/qSBNQh9AeaHDrq8I4kQQ1Cf+lAgx8fsacvSVCD0G/39A19SQJDX5JqpfqhXw7kZma/myJJfVf50F820OR44lm5kkRNQh9wBo8kUYPQP2M89B3Xl6TKh/6ygfZLdNqmJNUg9Md7+qOGviRVPvSXLWmHvj19SapD6LdK6DumL0nVD/0zSk//sLN3JKkGoT9gT1+SxnQd+hHRjIhvRcT/LPc3RsRTEbErIr4QEUtK+dJyf3dZv6HbbZ8KZ+9I0gkL0dP/BLCz4/6dwOcycxPwJnBTKb8JeDMzLwQ+V+otOmfvSNIJXYV+RKwD/jHw+XI/gF8GvlSqbAWuKcuby33K+stL/UXl7B1JOqHbnv7vA/8aGDtKei7wVmaOlvvDwNqyvBZ4HaCsP1DqL6rlpad/0NCXpPmHfkR8BNiXmTs6i6epmqewrvN5b46I7RGxfWRkZL7NG9dqNlg20OCdw6MnryxJFddNT//ngY9GxKvAQ7SHdX4fWBkRrVJnHbCnLA8D6wHK+nOA/ZOfNDPvycyhzBwaHBzsonknnLVsgLcPGfqSNO/Qz8xPZea6zNwAXAc8npkfA54Afr1U2wI8XJa3lfuU9Y9njy5yf9bSlj19SWJx5unfBtwaEbtpj9nfW8rvBc4t5bcCty/Ctqd15rIW7xw62qvNSdJpq3XyKieXmV8Hvl6WXwEunabOIeDahdjeXJ1pT1+SgBqckQvt0HdMX5LqEvrL7OlLEtQk9D2QK0lttQj99oHcUXo0WUiSTlv1CP2lA4weTw6PenllSfVWj9Bf1p6k5MFcSXVXi9A/a2k79B3Xl1R3tQj9M5eO9fQ9QUtSvdUi9M9ZPgDAWwcNfUn1VovQX7V8CQBvHjzS55ZIUn/VIvTPXdEO/f3vGvqS6q0WoX/2GQM0wtCXpFqEfrMRrFy+xNCXVHu1CH2A1SuWOKYvqfbqE/rLl/DDdwx9SfVWm9BftWLAnr6k2qtN6K9e4Zi+JNUm9M9dsZT97x7h2HGvtCmpvmoT+uefs4zjCSNvH+53UySpb2oT+mtXLgNgz4Ef97klktQ/tQn9C845A4A9bxn6kuqrNqH/npXt0N/71qE+t0SS+qc2oX/2shYrljQd3pFUa7UJ/YjgPSvPcHhHUq3VJvQB1q9ezvd/eLDfzZCkvqlV6G8670xeGXmX0WP+g3RJ9VSv0D//LI4cO87399vbl1RPtQr9951/JgC73ninzy2RpP6oVehfeN6ZRMDLP3i7302RpL6oVegvX9Li/eefxY7X3ux3UySpL2oV+gB/f8Nqdry634O5kmpp3qEfEesj4omI2BkRL0bEJ0r56oh4NCJ2ldtVpTwi4q6I2B0Rz0XEJQv1Iubi0o2reffIMV7Y86N+bF6S+qqbnv4o8C8z86eBy4BbIuIi4HbgsczcBDxW7gNcBWwqPzcDd3ex7Xn74IVraDWCrz6/tx+bl6S+mnfoZ+bezHy6LL8N7ATWApuBraXaVuCasrwZuD/bngRWRsQF8275PK1asYR/+L5B/vzZPV5bX1LtLMiYfkRsAC4GngLOz8y90P5iAM4r1dYCr3c8bLiU9dy1Q+vZc+AQf/7snn5sXpL6puvQj4gzgT8FPpmZsw2UxzRlU7raEXFzRGyPiO0jIyPdNm9aV1x0Pj99wdn87l+8xIGDRxdlG5J0Ouoq9CNigHbgP5CZXy7Fb4wN25TbfaV8GFjf8fB1wJSudmbek5lDmTk0ODjYTfNm1GgE//HX/g773j7Mx7d+0/+dK6k2upm9E8C9wM7M/GzHqm3AlrK8BXi4o/yGMovnMuDA2DBQP/zs+pXcdf3FPD98gF/6zNf5D1/ZyV/uGuGNHx3iqNM5JVVUZM7vYGZEfBD4S+B5YCwl/w3tcf0vAj8BvAZcm5n7y5fEHwJXAgeBGzNz+2zbGBoayu3bZ63StZd/8DafeeRlnnhpH6MdB3bPWtpi6UCDZiNoNRq0mkEzYvpBqklOoUq7Xpy85qk+l6Rq+akLzuY/X3/xvB4bETsyc2i6da35Nigz/w8zZ9Ll09RP4Jb5bm+xvP9vncUf3zDEgYNHeXHPAXaPvMP+d4/w1sGjHD12nNFjyejx5Njx4xO+FGZyyl+hp1AxT/3ZJFXM+lVnLMrzzjv0q+ac5QN84MI1fODCNf1uiiQtmtpdhkGS6szQl6QaMfQlqUYMfUmqEUNfkmrE0JekGjH0JalGDH1JqpF5X4ahFyJiBPh+F0+xBvibBWrOQrJdc2O75sZ2zU0V2/WTmTntFStP69DvVkRsn+n6E/1ku+bGds2N7ZqburXL4R1JqhFDX5JqpOqhf0+/GzAD2zU3tmtubNfc1KpdlR7TlyRNVPWeviSpQyVDPyKujIiXI2J3RNze422vj4gnImJnRLwYEZ8o5b8dEX8dEc+Un6s7HvOp0taXI+LDi9i2VyPi+bL97aVsdUQ8GhG7yu2qUh4RcVdp13MRcckiten9HfvkmYj4UUR8sh/7KyLui4h9EfFCR9mc909EbCn1d0XElum2tQDt+r2IeKls+88iYmUp3xARP+7Yb3/U8Zi/V97/3aXtXf9jthnaNuf3bqF/Z2do1xc62vRqRDxTynuyz2bJht5+xjKzUj9AE/gu8F5gCfAscFEPt38BcElZPgv4DnAR8NvAv5qm/kWljUuBjaXtzUVq26vAmkllvwvcXpZvB+4sy1cDX6X939EuA57q0Xv3A+An+7G/gA8BlwAvzHf/AKuBV8rtqrK8ahHadQXQKst3drRrQ2e9Sc/zV8DPlTZ/FbhqkfbZnN67xfidna5dk9b/J+Df9nKfzZINPf2MVbGnfymwOzNfycwjwEPA5l5tPDP3ZubTZfltYCewdpaHbAYeyszDmfk9YDft19Arm4GtZXkrcE1H+f3Z9iSwMiIuWOS2XA58NzNnOyFv0fZXZn4D2D/N9uayfz4MPJqZ+zPzTeBR2v8XekHblZmPZOZoufsksG625yhtOzsz/2+2k+P+jteyoG2bxUzv3YL/zs7WrtJb/2fAg7M9x0Lvs1myoaefsSqG/lrg9Y77w8weuosmIjYAF9P+Z/EAv1n+TLtv7E84etveBB6JiB0RcXMpOz8z90L7Qwmc14d2jbmOib+I/d5fMPf904/99nHaPcIxGyPiWxHxvyPiF0rZ2tKWXrVrLu9dr/fZLwBvZOaujrKe7rNJ2dDTz1gVQ3+6MbeeT1GKiDOBPwU+mZk/Au4G/jbws8Be2n9eQm/b+/OZeQlwFXBLRHxolro93Y8RsQT4KPA/StHpsL9mM1M7er3fPg2MAg+Uor3AT2TmxcCtwH+PiLN73K65vne9fk+vZ2Lnoqf7bJpsmLHqDNvvql1VDP1hYH3H/XXAnl42ICIGaL+pD2TmlwEy843MPJaZx4E/5sSQRM/am5l7yu0+4M9KG94YG7Ypt/t63a7iKuDpzHyjtLHv+6uY6/7pWfvKAbyPAB8rww+UoZMfluUdtMfK31fa1TkEtJifs7m+d73cZy3g14AvdLS3Z/tsumygx5+xKob+N4FNEbGx9B6vA7b1auNlvPBeYGdmfrajvHM8/J8AY7MKtgHXRcTSiNgIbKJ98Gih27UiIs4aW6Z9IPCFsv2xo/9bgIc72nVDmUFwGXBg7E/QRTKh99Xv/dVhrvvna8AVEbGqDGtcUcoWVERcCdwGfDQzD3aUD0ZEsyy/l/b+eaW07e2IuKx8Rm/oeC0L3ba5vne9/J39FeClzBwftunVPpspG+j1Z2y+R6JP5x/aR72/Q/sb+9M93vYHaf+p9RzwTPm5GvhvwPOlfBtwQcdjPl3a+jILMKNihna9l/asiGeBF8f2C3Au8Biwq9yuLuUB/JfSrueBoUXcZ8uBHwLndJT1fH/R/tLZCxyl3Zu6aT77h/YY++7yc+MitWs37XHdsc/YH5W6/7S8v88CTwO/2vE8Q7QD+LvAH1JOzlyEts35vVvo39np2lXK/wT4jUl1e7LPmDkbevoZ84xcSaqRKg7vSJJmYOhLUo0Y+pJUI4a+JNWIoS9JNWLoS1KNGPqSVCOGviTVyP8DU0Klf8MbE0EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets apply the update function iteratively (several epochs)\n",
    "start = time.time()\n",
    "learning_rate = 0.001 # the small amount by which we adjust the regression coefficients in each update\n",
    "num_epochs = 2000\n",
    "gamma = 2\n",
    "\n",
    "params_ridge = jax.random.normal(key, shape=(J, ))\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss, params_ridge = update(params_ridge, y, X, learning_rate, gamma)\n",
    "    losses.append(loss)\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Loss in epoch {epoch}: {loss}\")\n",
    "        \n",
    "training_duration_jit = np.round(time.time() - start, 2)\n",
    "print(f\"Total training time: {training_duration} seconds\")\n",
    "# plot the loss\n",
    "plt.plot(list(range(num_epochs)), losses[0:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True betas: [4 3 2 6 0 0] \n",
      " Gradient Descent OLS Betas: [ 3.9455807   2.9163518   1.9742827   5.965656    0.02850169 -0.4015602 ] \n",
      " Gradient Descent Ridge Betas: [ 3.511255    2.5841231   1.721271    5.3111634   0.03922734 -0.16204447]\n"
     ]
    }
   ],
   "source": [
    "print(f'True betas: {betas} \\n Gradient Descent OLS Betas: {params} \\n Gradient Descent Ridge Betas: {params_ridge}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shrinkage compared to the OLS coefficients.  We also verify that our gradient-based approximations correpond to the analytic solution for the Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.51125199  2.58411979  1.7212693   5.31115745  0.03922761 -0.162046  ]\n"
     ]
    }
   ],
   "source": [
    "temp = np.ones(6) * gamma\n",
    "print(np.dot(np.linalg.inv(np.dot(X.T,X) + 1000 * np.diag(temp)),np.dot(X.T,y))) # numpy for (X^X + N * gamma * I)^{-1}X^Ty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
